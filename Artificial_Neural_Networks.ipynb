{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score,r2_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/variables3.csv')\n",
    "\n",
    "X = df.drop('MR',axis=1)\n",
    "y = df['MR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt_sht</th>\n",
       "      <th>vel_sht</th>\n",
       "      <th>pit_sht</th>\n",
       "      <th>alt_tgt</th>\n",
       "      <th>vel_tgt</th>\n",
       "      <th>sin(hdg_tgt)</th>\n",
       "      <th>cos(hdg_tgt)</th>\n",
       "      <th>sin(rgt_tgt)</th>\n",
       "      <th>cos(rgt_tgt)</th>\n",
       "      <th>MR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44421.970347</td>\n",
       "      <td>537.496000</td>\n",
       "      <td>-24.833196</td>\n",
       "      <td>34353.468266</td>\n",
       "      <td>541.824384</td>\n",
       "      <td>-0.967068</td>\n",
       "      <td>-0.254518</td>\n",
       "      <td>0.305089</td>\n",
       "      <td>0.952324</td>\n",
       "      <td>26.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44548.033860</td>\n",
       "      <td>538.187792</td>\n",
       "      <td>10.192052</td>\n",
       "      <td>32946.738582</td>\n",
       "      <td>583.690520</td>\n",
       "      <td>0.782715</td>\n",
       "      <td>0.622380</td>\n",
       "      <td>-0.164024</td>\n",
       "      <td>0.986456</td>\n",
       "      <td>20.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44352.491182</td>\n",
       "      <td>591.255827</td>\n",
       "      <td>-22.500498</td>\n",
       "      <td>27989.703128</td>\n",
       "      <td>590.506078</td>\n",
       "      <td>-0.982489</td>\n",
       "      <td>-0.186323</td>\n",
       "      <td>-0.292326</td>\n",
       "      <td>0.956319</td>\n",
       "      <td>13.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44426.946447</td>\n",
       "      <td>528.648879</td>\n",
       "      <td>-41.096294</td>\n",
       "      <td>41864.869016</td>\n",
       "      <td>518.200251</td>\n",
       "      <td>-0.828868</td>\n",
       "      <td>0.559444</td>\n",
       "      <td>-0.376352</td>\n",
       "      <td>0.926477</td>\n",
       "      <td>9.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44393.321784</td>\n",
       "      <td>591.514922</td>\n",
       "      <td>-20.926105</td>\n",
       "      <td>19894.116005</td>\n",
       "      <td>567.848255</td>\n",
       "      <td>-0.328344</td>\n",
       "      <td>-0.944558</td>\n",
       "      <td>0.814632</td>\n",
       "      <td>0.579978</td>\n",
       "      <td>16.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5815</th>\n",
       "      <td>44441.578213</td>\n",
       "      <td>598.613378</td>\n",
       "      <td>-6.756109</td>\n",
       "      <td>42934.233400</td>\n",
       "      <td>442.350927</td>\n",
       "      <td>-0.998991</td>\n",
       "      <td>0.044913</td>\n",
       "      <td>0.402013</td>\n",
       "      <td>0.915634</td>\n",
       "      <td>33.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5816</th>\n",
       "      <td>44678.316987</td>\n",
       "      <td>593.082438</td>\n",
       "      <td>6.748711</td>\n",
       "      <td>24093.609529</td>\n",
       "      <td>576.417176</td>\n",
       "      <td>0.491367</td>\n",
       "      <td>0.870953</td>\n",
       "      <td>-0.305427</td>\n",
       "      <td>0.952216</td>\n",
       "      <td>15.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817</th>\n",
       "      <td>44897.188245</td>\n",
       "      <td>594.954717</td>\n",
       "      <td>-44.614118</td>\n",
       "      <td>36910.748896</td>\n",
       "      <td>556.797646</td>\n",
       "      <td>-0.930452</td>\n",
       "      <td>0.366415</td>\n",
       "      <td>0.679538</td>\n",
       "      <td>0.733640</td>\n",
       "      <td>12.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5818</th>\n",
       "      <td>44219.306776</td>\n",
       "      <td>543.233281</td>\n",
       "      <td>40.566989</td>\n",
       "      <td>44279.634058</td>\n",
       "      <td>593.559503</td>\n",
       "      <td>0.788823</td>\n",
       "      <td>0.614620</td>\n",
       "      <td>-0.741844</td>\n",
       "      <td>0.670573</td>\n",
       "      <td>27.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>44374.791861</td>\n",
       "      <td>586.867979</td>\n",
       "      <td>-30.308584</td>\n",
       "      <td>38130.293320</td>\n",
       "      <td>449.982685</td>\n",
       "      <td>0.638752</td>\n",
       "      <td>-0.769413</td>\n",
       "      <td>0.841625</td>\n",
       "      <td>0.540062</td>\n",
       "      <td>13.671875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5820 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           alt_sht     vel_sht    pit_sht       alt_tgt     vel_tgt  \\\n",
       "0     44421.970347  537.496000 -24.833196  34353.468266  541.824384   \n",
       "1     44548.033860  538.187792  10.192052  32946.738582  583.690520   \n",
       "2     44352.491182  591.255827 -22.500498  27989.703128  590.506078   \n",
       "3     44426.946447  528.648879 -41.096294  41864.869016  518.200251   \n",
       "4     44393.321784  591.514922 -20.926105  19894.116005  567.848255   \n",
       "...            ...         ...        ...           ...         ...   \n",
       "5815  44441.578213  598.613378  -6.756109  42934.233400  442.350927   \n",
       "5816  44678.316987  593.082438   6.748711  24093.609529  576.417176   \n",
       "5817  44897.188245  594.954717 -44.614118  36910.748896  556.797646   \n",
       "5818  44219.306776  543.233281  40.566989  44279.634058  593.559503   \n",
       "5819  44374.791861  586.867979 -30.308584  38130.293320  449.982685   \n",
       "\n",
       "      sin(hdg_tgt)  cos(hdg_tgt)  sin(rgt_tgt)  cos(rgt_tgt)         MR  \n",
       "0        -0.967068     -0.254518      0.305089      0.952324  26.328125  \n",
       "1         0.782715      0.622380     -0.164024      0.986456  20.390625  \n",
       "2        -0.982489     -0.186323     -0.292326      0.956319  13.359375  \n",
       "3        -0.828868      0.559444     -0.376352      0.926477   9.609375  \n",
       "4        -0.328344     -0.944558      0.814632      0.579978  16.953125  \n",
       "...            ...           ...           ...           ...        ...  \n",
       "5815     -0.998991      0.044913      0.402013      0.915634  33.203125  \n",
       "5816      0.491367      0.870953     -0.305427      0.952216  15.078125  \n",
       "5817     -0.930452      0.366415      0.679538      0.733640  12.578125  \n",
       "5818      0.788823      0.614620     -0.741844      0.670573  27.734375  \n",
       "5819      0.638752     -0.769413      0.841625      0.540062  13.671875  \n",
       "\n",
       "[5820 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and  K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 46.4753 - val_loss: 14.1451\n",
      "Epoch 2/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 18.7561 - val_loss: 11.5370\n",
      "Epoch 3/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 16.2861 - val_loss: 12.5439\n",
      "Epoch 4/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 10.9287 - val_loss: 10.7233\n",
      "Epoch 5/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 10.3749 - val_loss: 7.7063\n",
      "Epoch 6/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 8.7978 - val_loss: 7.2901\n",
      "Epoch 7/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 8.8047 - val_loss: 5.9773\n",
      "Epoch 8/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 8.5245 - val_loss: 15.4036\n",
      "Epoch 9/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.5459 - val_loss: 12.2512\n",
      "Epoch 10/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.0618 - val_loss: 7.4542\n",
      "Epoch 11/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.3119 - val_loss: 6.4609\n",
      "Epoch 12/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.9946 - val_loss: 5.2346\n",
      "Epoch 13/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.5204 - val_loss: 5.5065\n",
      "Epoch 14/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.4589 - val_loss: 7.9664\n",
      "Epoch 15/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.2141 - val_loss: 5.2628\n",
      "Epoch 16/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.2875 - val_loss: 6.8613\n",
      "Epoch 17/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.9579 - val_loss: 12.3578\n",
      "Epoch 18/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.9463 - val_loss: 5.4412\n",
      "Epoch 19/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.9395 - val_loss: 6.2697\n",
      "Epoch 20/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.8754 - val_loss: 5.0758\n",
      "Epoch 21/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.6972 - val_loss: 7.2530\n",
      "Epoch 22/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.7344 - val_loss: 4.8537\n",
      "Epoch 23/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.5053 - val_loss: 12.7028\n",
      "Epoch 24/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.7626 - val_loss: 6.5531\n",
      "Epoch 25/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.9858 - val_loss: 5.2825\n",
      "Epoch 26/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.8940 - val_loss: 5.1626\n",
      "Epoch 27/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.3074 - val_loss: 4.5684\n",
      "Epoch 28/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.0252 - val_loss: 4.7355\n",
      "Epoch 29/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.4021 - val_loss: 6.3319\n",
      "Epoch 30/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.0634 - val_loss: 4.1853\n",
      "Epoch 31/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.6727 - val_loss: 4.5529\n",
      "Epoch 32/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.4220 - val_loss: 4.5147\n",
      "Epoch 33/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.2810 - val_loss: 5.9595\n",
      "Epoch 34/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.1301 - val_loss: 4.8070\n",
      "Epoch 35/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.5023 - val_loss: 5.8731\n",
      "Epoch 36/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.7914 - val_loss: 4.9434\n",
      "Epoch 37/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.6538 - val_loss: 5.3102\n",
      "Epoch 38/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.3470 - val_loss: 4.6731\n",
      "Epoch 39/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.0435 - val_loss: 5.2979\n",
      "Epoch 40/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.1974 - val_loss: 4.2709\n",
      "Epoch 41/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.2462 - val_loss: 4.8886\n",
      "Epoch 42/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.1654 - val_loss: 6.5982\n",
      "Epoch 43/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.9453 - val_loss: 4.4594\n",
      "Epoch 44/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.1556 - val_loss: 4.3496\n",
      "Epoch 45/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.0101 - val_loss: 5.4405\n",
      "Epoch 46/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.6901 - val_loss: 4.4128\n",
      "Epoch 47/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.7957 - val_loss: 4.5977\n",
      "Epoch 48/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.3087 - val_loss: 4.9251\n",
      "Epoch 49/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.6924 - val_loss: 4.2187\n",
      "Epoch 50/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.3130 - val_loss: 4.7828\n",
      "Epoch 51/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.1517 - val_loss: 4.9765\n",
      "Epoch 52/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.3953 - val_loss: 4.3066\n",
      "Epoch 53/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.9902 - val_loss: 5.0289\n",
      "Epoch 54/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.7503 - val_loss: 6.5081\n",
      "Epoch 55/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.5510 - val_loss: 5.1270\n",
      "Epoch 56/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.5367 - val_loss: 3.6003\n",
      "Epoch 57/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.0850 - val_loss: 4.8365\n",
      "Epoch 58/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.5614 - val_loss: 5.0799\n",
      "Epoch 59/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.0266 - val_loss: 4.4954\n",
      "Epoch 60/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.4768 - val_loss: 4.2172\n",
      "Epoch 61/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.6215 - val_loss: 4.0776\n",
      "Epoch 62/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.3194 - val_loss: 4.2648\n",
      "Epoch 63/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.1334 - val_loss: 3.8718\n",
      "Epoch 64/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.2392 - val_loss: 4.2143\n",
      "Epoch 65/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.8389 - val_loss: 4.3639\n",
      "Epoch 66/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.3575 - val_loss: 4.9111\n",
      "Epoch 67/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.6058 - val_loss: 6.1102\n",
      "Epoch 68/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.2527 - val_loss: 4.3733\n",
      "Epoch 69/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.0662 - val_loss: 3.6224\n",
      "Epoch 70/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.9599 - val_loss: 4.7296\n",
      "Epoch 71/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.4369 - val_loss: 3.7353\n",
      "Epoch 72/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.1366 - val_loss: 4.8746\n",
      "Epoch 73/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.9843 - val_loss: 3.8146\n",
      "Epoch 74/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.7244 - val_loss: 8.1092\n",
      "Epoch 75/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.4086 - val_loss: 4.2216\n",
      "Epoch 76/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.8472 - val_loss: 3.7801\n",
      "Epoch 77/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 4.0174 - val_loss: 4.5848\n",
      "Epoch 78/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.6327 - val_loss: 3.5141\n",
      "Epoch 79/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.3976 - val_loss: 4.1280\n",
      "Epoch 80/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.6687 - val_loss: 4.4824\n",
      "Epoch 81/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.7564 - val_loss: 4.3851\n",
      "Epoch 82/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.7829 - val_loss: 4.0935\n",
      "Epoch 83/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.1231 - val_loss: 4.6780\n",
      "Epoch 84/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.8541 - val_loss: 4.2426\n",
      "Epoch 85/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.5064 - val_loss: 4.0996\n",
      "Epoch 86/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.4688 - val_loss: 4.0552\n",
      "Epoch 87/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.9946 - val_loss: 4.6474\n",
      "Epoch 88/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.7160 - val_loss: 3.9156\n",
      "Epoch 89/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.5594 - val_loss: 3.5675\n",
      "Epoch 90/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.0922 - val_loss: 3.9967\n",
      "Epoch 91/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.8561 - val_loss: 4.9404\n",
      "Epoch 92/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.8313 - val_loss: 4.2118\n",
      "Epoch 93/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.3661 - val_loss: 4.4849\n",
      "Epoch 94/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.5041 - val_loss: 3.8236\n",
      "Epoch 95/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.8424 - val_loss: 4.1954\n",
      "Epoch 96/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.0577 - val_loss: 4.4422\n",
      "Epoch 97/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.4734 - val_loss: 3.8377\n",
      "Epoch 98/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.7551 - val_loss: 4.3879\n",
      "Epoch 99/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.5192 - val_loss: 5.7819\n",
      "Epoch 100/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.0262 - val_loss: 4.1813\n",
      "Epoch 101/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.5016 - val_loss: 4.0739\n",
      "Epoch 102/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2899 - val_loss: 4.7863\n",
      "Epoch 103/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.0457 - val_loss: 4.8765\n",
      "Epoch 104/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.1360 - val_loss: 4.2945\n",
      "Epoch 105/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.8599 - val_loss: 4.4127\n",
      "Epoch 106/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.4494 - val_loss: 4.5866\n",
      "Epoch 107/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2527 - val_loss: 5.4171\n",
      "Epoch 108/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.4377 - val_loss: 5.2317\n",
      "Epoch 109/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1405 - val_loss: 4.0293\n",
      "Epoch 110/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.4294 - val_loss: 3.6941\n",
      "Epoch 111/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.3253 - val_loss: 4.4717\n",
      "Epoch 112/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.9142 - val_loss: 4.1172\n",
      "Epoch 113/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.5679 - val_loss: 4.6287\n",
      "Epoch 114/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2070 - val_loss: 3.9151\n",
      "Epoch 115/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.6526 - val_loss: 5.6514\n",
      "Epoch 116/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.3839 - val_loss: 3.8035\n",
      "Epoch 117/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1875 - val_loss: 3.9015\n",
      "Epoch 118/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.1757 - val_loss: 7.0523\n",
      "Epoch 119/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.5338 - val_loss: 3.6868\n",
      "Epoch 120/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.4155 - val_loss: 3.8029\n",
      "Epoch 121/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.6042 - val_loss: 11.8259\n",
      "Epoch 122/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.6949 - val_loss: 3.6188\n",
      "Epoch 123/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.4180 - val_loss: 3.4353\n",
      "Epoch 124/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1320 - val_loss: 4.5705\n",
      "Epoch 125/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.5516 - val_loss: 4.2088\n",
      "Epoch 126/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1473 - val_loss: 4.8915\n",
      "Epoch 127/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.7362 - val_loss: 4.0260\n",
      "Epoch 128/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.4673 - val_loss: 4.2926\n",
      "Epoch 129/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0419 - val_loss: 3.9195\n",
      "Epoch 130/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0979 - val_loss: 3.2275\n",
      "Epoch 131/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.7004 - val_loss: 4.4960\n",
      "Epoch 132/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0188 - val_loss: 3.8245\n",
      "Epoch 133/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.3150 - val_loss: 3.4874\n",
      "Epoch 134/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.9157 - val_loss: 4.4205\n",
      "Epoch 135/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2972 - val_loss: 4.2904\n",
      "Epoch 136/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.9622 - val_loss: 3.8275\n",
      "Epoch 137/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0243 - val_loss: 17.4852\n",
      "Epoch 138/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.4846 - val_loss: 4.2494\n",
      "Epoch 139/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0671 - val_loss: 4.1343\n",
      "Epoch 140/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1234 - val_loss: 3.7711\n",
      "Epoch 141/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1733 - val_loss: 3.9810\n",
      "Epoch 142/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6796 - val_loss: 4.2145\n",
      "Epoch 143/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6829 - val_loss: 4.1877\n",
      "Epoch 144/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0629 - val_loss: 4.6796\n",
      "Epoch 145/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2875 - val_loss: 4.2455\n",
      "Epoch 146/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2242 - val_loss: 6.5192\n",
      "Epoch 147/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.5787 - val_loss: 3.8392\n",
      "Epoch 148/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.3744 - val_loss: 3.9555\n",
      "Epoch 149/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2667 - val_loss: 4.5317\n",
      "Epoch 150/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0582 - val_loss: 3.6230\n",
      "Epoch 151/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8888 - val_loss: 4.7203\n",
      "Epoch 152/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5217 - val_loss: 4.0222\n",
      "Epoch 153/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.9960 - val_loss: 4.5367\n",
      "Epoch 154/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.5910 - val_loss: 4.4173\n",
      "Epoch 155/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8176 - val_loss: 3.9110\n",
      "Epoch 156/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0834 - val_loss: 4.2253\n",
      "Epoch 157/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.5157 - val_loss: 3.8256\n",
      "Epoch 158/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1948 - val_loss: 3.5756\n",
      "Epoch 159/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 3.3003 - val_loss: 4.4169\n",
      "Epoch 160/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8013 - val_loss: 3.9622\n",
      "Epoch 161/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7457 - val_loss: 4.2304\n",
      "Epoch 162/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7381 - val_loss: 3.9940\n",
      "Epoch 163/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8207 - val_loss: 4.0864\n",
      "Epoch 164/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.9753 - val_loss: 3.7732\n",
      "Epoch 165/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6670 - val_loss: 3.6842\n",
      "Epoch 166/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2111 - val_loss: 4.8008\n",
      "Epoch 167/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.9642 - val_loss: 3.8127\n",
      "Epoch 168/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1512 - val_loss: 4.2435\n",
      "Epoch 169/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8452 - val_loss: 3.9005\n",
      "Epoch 170/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8829 - val_loss: 4.4521\n",
      "Epoch 171/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6969 - val_loss: 3.6307\n",
      "Epoch 172/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.9618 - val_loss: 3.8934\n",
      "Epoch 173/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0797 - val_loss: 3.5322\n",
      "Epoch 174/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.8408 - val_loss: 4.2598\n",
      "Epoch 175/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2463 - val_loss: 4.3180\n",
      "Epoch 176/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8024 - val_loss: 4.1364\n",
      "Epoch 177/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7612 - val_loss: 8.1965\n",
      "Epoch 178/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8444 - val_loss: 4.1403\n",
      "Epoch 179/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8083 - val_loss: 3.6968\n",
      "Epoch 180/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5498 - val_loss: 3.7490\n",
      "Epoch 181/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6709 - val_loss: 4.2666\n",
      "Epoch 182/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8204 - val_loss: 4.0001\n",
      "Epoch 183/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6613 - val_loss: 3.8836\n",
      "Epoch 184/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7029 - val_loss: 3.8098\n",
      "Epoch 185/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.9276 - val_loss: 5.1290\n",
      "Epoch 186/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7378 - val_loss: 4.5656\n",
      "Epoch 187/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.9088 - val_loss: 3.8664\n",
      "Epoch 188/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.0205 - val_loss: 3.4339\n",
      "Epoch 189/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.3620 - val_loss: 3.4005\n",
      "Epoch 190/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7050 - val_loss: 3.7504\n",
      "Epoch 191/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5181 - val_loss: 3.9047\n",
      "Epoch 192/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1602 - val_loss: 4.1061\n",
      "Epoch 193/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8031 - val_loss: 3.5794\n",
      "Epoch 194/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6104 - val_loss: 3.7363\n",
      "Epoch 195/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6219 - val_loss: 3.6768\n",
      "Epoch 196/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6434 - val_loss: 3.9570\n",
      "Epoch 197/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7649 - val_loss: 4.3904\n",
      "Epoch 198/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.1981 - val_loss: 3.9809\n",
      "Epoch 199/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7558 - val_loss: 4.1443\n",
      "Epoch 200/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8783 - val_loss: 3.2732\n",
      "Epoch 201/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5478 - val_loss: 4.7266\n",
      "Epoch 202/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7930 - val_loss: 4.0553\n",
      "Epoch 203/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5135 - val_loss: 4.1584\n",
      "Epoch 204/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6209 - val_loss: 3.6253\n",
      "Epoch 205/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2289 - val_loss: 3.6405\n",
      "Epoch 206/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5874 - val_loss: 3.9967\n",
      "Epoch 207/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5090 - val_loss: 4.4290\n",
      "Epoch 208/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.4042 - val_loss: 5.6200\n",
      "Epoch 209/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.6146 - val_loss: 3.6907\n",
      "Epoch 210/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.3843 - val_loss: 5.1188\n",
      "Epoch 211/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7568 - val_loss: 3.5412\n",
      "Epoch 212/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.2560 - val_loss: 3.3882\n",
      "Epoch 213/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8622 - val_loss: 4.0556\n",
      "Epoch 214/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.3416 - val_loss: 3.7312\n",
      "Epoch 215/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.4794 - val_loss: 3.4403\n",
      "Epoch 216/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.9787 - val_loss: 3.5969\n",
      "Epoch 217/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.7941 - val_loss: 3.5939\n",
      "Epoch 218/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.4607 - val_loss: 4.1395\n",
      "Epoch 219/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5240 - val_loss: 3.6513\n",
      "Epoch 220/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.2932 - val_loss: 3.5760\n",
      "Epoch 221/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5385 - val_loss: 4.1655\n",
      "Epoch 222/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.1686 - val_loss: 4.0689\n",
      "Epoch 223/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.4129 - val_loss: 3.5692\n",
      "Epoch 224/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.4471 - val_loss: 4.8405\n",
      "Epoch 225/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5538 - val_loss: 3.9783\n",
      "Epoch 226/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5250 - val_loss: 4.8755\n",
      "Epoch 227/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.8582 - val_loss: 3.7656\n",
      "Epoch 228/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.5314 - val_loss: 3.7594\n",
      "Epoch 229/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 1.1989 - val_loss: 4.1513\n",
      "Epoch 230/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 2.4585 - val_loss: 4.0240\n",
      "Epoch 00230: early stopping\n",
      "------------------------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 47.7097 - val_loss: 21.8353\n",
      "Epoch 2/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 19.5702 - val_loss: 17.3773\n",
      "Epoch 3/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 15.5485 - val_loss: 18.4811\n",
      "Epoch 4/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 12.4417 - val_loss: 10.0603\n",
      "Epoch 5/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 9.6977 - val_loss: 7.1024\n",
      "Epoch 6/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 9.3831 - val_loss: 7.5068\n",
      "Epoch 7/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 9.0702 - val_loss: 8.3385\n",
      "Epoch 8/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.5443 - val_loss: 6.3092\n",
      "Epoch 9/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 9.5989 - val_loss: 7.4737\n",
      "Epoch 10/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 8.1554 - val_loss: 6.2801\n",
      "Epoch 11/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.3140 - val_loss: 6.1852\n",
      "Epoch 12/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.1051 - val_loss: 7.5746\n",
      "Epoch 13/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.7655 - val_loss: 5.9222\n",
      "Epoch 14/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.6326 - val_loss: 6.0821\n",
      "Epoch 15/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.8378 - val_loss: 6.0196\n",
      "Epoch 16/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.7847 - val_loss: 6.6570\n",
      "Epoch 17/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.9052 - val_loss: 6.0217\n",
      "Epoch 18/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.7184 - val_loss: 6.5628\n",
      "Epoch 19/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.9750 - val_loss: 6.2471\n",
      "Epoch 20/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.3447 - val_loss: 7.8251\n",
      "Epoch 21/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 7.6106 - val_loss: 4.7370\n",
      "Epoch 22/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.3967 - val_loss: 6.3792\n",
      "Epoch 23/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.4791 - val_loss: 6.2275\n",
      "Epoch 24/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.3643 - val_loss: 7.5882\n",
      "Epoch 25/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.0176 - val_loss: 6.0730\n",
      "Epoch 26/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.1606 - val_loss: 6.1478\n",
      "Epoch 27/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.0649 - val_loss: 5.9283\n",
      "Epoch 28/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 6.5023 - val_loss: 8.2275\n",
      "Epoch 29/1000\n",
      "4656/4656 [==============================] - 8s 2ms/step - loss: 5.6515 - val_loss: 5.1073\n",
      "Epoch 30/1000\n",
      "1996/4656 [===========>..................] - ETA: 4s - loss: 5.0696"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['EVS','R2','MAE','MSE','RMSE'])\n",
    "\n",
    "for train, test in kfold.split(X, y):\n",
    "\n",
    "  # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9,activation='relu'))\n",
    "    model.add(Dense(500,activation='relu'))\n",
    "    model.add(Dense(250,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dense(25,activation='relu'))\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    model.add(Dense(5,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    \n",
    "    # Early Stop\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    model.fit(X[train], \n",
    "              y[train],\n",
    "              epochs=1000,\n",
    "              batch_size=1,\n",
    "              validation_data=(X[test], y[test]), \n",
    "              callbacks=[early_stop])\n",
    "    \n",
    "    # make predictions for test data\n",
    "    predictions = model.predict(X[test])\n",
    "    \n",
    "    # Evaluation on Test Data\n",
    "    metrics = {'EVS' : explained_variance_score(y[test], predictions),\n",
    "               'R2'  : r2_score(y[test], predictions),\n",
    "               'MAE' : mean_absolute_error(y[test],predictions),\n",
    "               'MSE' : mean_squared_error(y[test],predictions),\n",
    "               'RMSE': np.sqrt(mean_squared_error(y[test],predictions))}\n",
    "\n",
    "    metrics = pd.DataFrame(metrics, index=['fold {}'.format(fold_no)])                          \n",
    "    \n",
    "    metrics_df = pd.concat([metrics_df,metrics], ignore_index=False)\n",
    "                               \n",
    "    metrics = pd.DataFrame(columns=['EVS','R2','MAE','MSE','RMSE'])\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Mean and Standard Deviation of the Maximum Range\n",
    "print(\"Mean of MR: {:.3f}\".format(y.mean()))\n",
    "print(\"STD of MR: {:.3f}\".format(y.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a Single New Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample = df.drop('MR',axis=1).iloc[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample = scaler.transform(single_sample.values.reshape(-1, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12522089, 0.02787258, 0.87193441, 0.031173  , 0.34594907,\n",
       "        0.02811896, 0.66531289, 0.91274298, 0.39841101]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523082"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(single_sample)\n",
    "predicted[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.078125"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = df['MR'].iloc[j]\n",
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8741832"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(real-predicted)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready Model for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'sequential_2/dense_19/kernel:0' shape=(9, 500) dtype=float32, numpy=\n",
       " array([[ 0.9161012 , -0.08981959, -0.01819481, ..., -0.01716812,\n",
       "         -0.28162777, -0.04081641],\n",
       "        [-0.20571229, -0.19958942, -0.15611082, ..., -0.38791233,\n",
       "          0.27529487, -0.0113493 ],\n",
       "        [-0.9495167 , -0.0769145 , -0.08025127, ..., -0.00800568,\n",
       "         -0.04797319, -0.02971826],\n",
       "        ...,\n",
       "        [ 0.14866439, -0.20049852, -0.3893773 , ...,  0.23266563,\n",
       "          0.36964425, -0.01897763],\n",
       "        [ 0.04790989, -0.18123464, -0.29524052, ..., -0.589157  ,\n",
       "         -0.09238208, -0.0255378 ],\n",
       "        [-0.64229834,  0.02261791,  0.00289192, ...,  0.74265736,\n",
       "          1.1804429 , -0.02199275]], dtype=float32)>,\n",
       " <tf.Variable 'sequential_2/dense_19/bias:0' shape=(500,) dtype=float32, numpy=\n",
       " array([ 1.36119932e-01, -8.57219696e-02,  5.25414608e-02, -3.72593403e-02,\n",
       "        -1.10155594e+00,  1.33359715e-01, -8.92129689e-02, -6.74826026e-01,\n",
       "        -3.28277409e-01, -2.37149019e-02, -7.34346509e-02, -6.68254942e-02,\n",
       "        -7.41788372e-02,  5.07535115e-02,  4.68555093e-02, -8.59528780e-01,\n",
       "        -1.15169045e-02, -8.65727738e-02, -7.46798813e-01, -9.63248163e-02,\n",
       "        -2.52953768e-02, -9.53543961e-01,  8.68717551e-01, -1.33762464e-01,\n",
       "        -5.55532984e-02, -2.83160567e-01, -7.73597956e-02, -8.74917060e-02,\n",
       "        -6.94731474e-02, -3.28761190e-02, -5.36563527e-03, -5.01474850e-02,\n",
       "        -2.99843047e-02,  5.52626193e-01, -8.09384137e-02, -6.88005090e-02,\n",
       "        -3.30927223e-02, -7.09416941e-02, -6.43173605e-02, -3.63581508e-01,\n",
       "         5.18534742e-02, -7.60577321e-02,  2.58813947e-01, -3.56963724e-02,\n",
       "         2.39551589e-02, -6.03163652e-02,  3.11240107e-02, -2.53382683e-01,\n",
       "        -8.59122649e-02, -5.66350259e-02,  4.54727188e-02, -3.89178284e-03,\n",
       "        -1.20238014e-01,  9.32544053e-01, -1.16984344e+00, -6.70063645e-02,\n",
       "        -5.54451346e-01,  5.30102074e-01, -2.85014920e-02, -1.23178296e-01,\n",
       "        -1.94334984e-02, -6.16232082e-02, -1.28686074e-02, -9.09587592e-02,\n",
       "        -9.71309841e-02, -9.29219723e-02, -7.23116636e-01, -7.63289332e-02,\n",
       "        -3.54323797e-02, -3.87679152e-02, -3.63025874e-01, -2.13386230e-02,\n",
       "        -1.67962667e-02, -6.87583506e-01,  1.97139218e-01, -8.76696467e-01,\n",
       "        -9.41491053e-02, -5.59474304e-02, -1.42915966e-02, -1.19561322e-01,\n",
       "        -9.72547531e-02, -4.17955741e-02, -6.57577738e-02,  3.70654970e-01,\n",
       "        -7.33160451e-02, -9.30648088e-01, -6.81603700e-02,  8.89035612e-02,\n",
       "        -4.43285033e-02, -3.39353174e-01,  0.00000000e+00, -4.66444492e-02,\n",
       "        -4.11313027e-02, -3.10593862e-02, -3.22364569e-02, -8.80664364e-02,\n",
       "        -9.08739716e-02,  9.06501472e-01, -5.50512224e-02,  9.92056906e-01,\n",
       "        -7.71763682e-01, -8.98546159e-01, -5.81786409e-02, -1.16856784e-01,\n",
       "        -4.02329028e-01,  1.53058544e-01, -1.07053317e-01, -8.40880945e-02,\n",
       "        -8.57000500e-02, -1.09507956e-01, -9.95950252e-02,  3.86345685e-01,\n",
       "        -1.07829309e+00, -1.06519729e-01, -2.59102099e-02, -9.76768315e-01,\n",
       "        -5.13759777e-02, -1.56897455e-01,  1.43063948e-01, -3.20457108e-02,\n",
       "        -4.03207898e-01, -1.19071174e+00, -1.20017424e-01, -8.35248306e-02,\n",
       "         2.77207959e-02, -9.10206258e-01, -5.88870943e-02, -5.41255549e-02,\n",
       "        -1.33524761e-01, -7.84870386e-02,  6.98582828e-02, -1.74517870e-01,\n",
       "         9.41534042e-01, -1.25360358e+00, -5.43036461e-02, -9.06923532e-01,\n",
       "        -1.39492482e-01, -5.49578182e-02, -1.91401571e-01,  2.90950835e-01,\n",
       "        -6.33229017e-02,  4.22129445e-02, -8.14499632e-02, -5.05461283e-02,\n",
       "        -8.87131274e-01,  1.42025769e-01, -1.26716778e-01, -1.16958931e-01,\n",
       "        -1.57906801e-01, -2.11566128e-02, -1.58623680e-02, -3.44749889e-03,\n",
       "        -5.52174486e-02, -6.60691187e-02, -8.07482228e-02, -6.32043555e-02,\n",
       "        -1.73833907e-01, -3.20469849e-02, -3.84027809e-02,  5.16688704e-01,\n",
       "        -2.90276613e-02, -7.77020231e-02, -5.60296178e-02, -8.83286655e-01,\n",
       "        -1.42676771e-01, -7.03476608e-01, -1.15633510e-01, -2.17477500e-01,\n",
       "        -5.54641150e-02, -1.59646243e-01, -5.72921187e-02, -9.73167196e-02,\n",
       "        -7.26242125e-01, -1.45718027e-02, -7.68124983e-02, -9.30579379e-02,\n",
       "        -6.88908100e-01, -4.84772325e-02, -6.02562688e-02, -9.34451893e-02,\n",
       "        -6.57210276e-02,  6.08717978e-01,  2.06800699e-02, -5.75711438e-03,\n",
       "        -1.09954089e-01, -2.61067927e-01, -5.04749835e-01,  1.00206450e-01,\n",
       "         1.39604285e-01, -1.33606330e-01, -1.59884229e-01, -1.18081808e-01,\n",
       "        -6.59172609e-02, -6.46269545e-02, -2.64799614e-02,  1.65469535e-02,\n",
       "        -3.19521204e-02,  5.82542777e-01, -4.04744558e-02, -3.43098640e-02,\n",
       "        -4.94715311e-02, -2.11698040e-02, -2.78251559e-01, -2.68027067e-01,\n",
       "         1.68140614e-04, -6.31092846e-01, -4.47902568e-02, -1.98459938e-01,\n",
       "        -2.29056589e-02,  0.00000000e+00, -3.47253755e-02,  0.00000000e+00,\n",
       "        -8.03630427e-03, -7.23588839e-02, -2.74403766e-02, -2.39971783e-02,\n",
       "        -2.26688907e-02, -3.02930549e-02, -8.83561671e-02,  5.37121117e-01,\n",
       "        -2.48720214e-01,  9.60012317e-01, -1.12359509e-01,  7.88952768e-01,\n",
       "        -2.22778916e-02, -1.75892934e-01, -1.43991709e-01, -3.35733965e-02,\n",
       "        -1.19829401e-01, -2.38912646e-03, -4.93911207e-02, -1.53491544e-02,\n",
       "         5.56924120e-02, -2.65242383e-02, -1.12807527e-01, -1.21588580e-01,\n",
       "        -7.74117187e-02, -3.92288901e-02, -7.11075664e-02, -7.83799365e-02,\n",
       "        -8.09360445e-02, -1.75116793e-03, -3.13551091e-02, -8.39018822e-02,\n",
       "        -3.69050168e-02, -1.49960304e-02, -2.10328504e-01, -1.57086886e-02,\n",
       "        -2.36054268e-02, -3.09456550e-02, -1.55359745e-01, -3.04625511e-01,\n",
       "        -4.13518734e-02, -5.98620586e-02, -1.36832194e-03, -2.50689872e-02,\n",
       "        -5.70490174e-02, -4.06055868e-01, -7.74152130e-02, -3.89529467e-02,\n",
       "        -8.92554969e-02, -2.82264560e-01, -7.36671537e-02, -4.32947278e-02,\n",
       "        -1.73763186e-01,  3.30352634e-01, -1.44883141e-01, -6.11498117e-01,\n",
       "         4.68179822e-01, -2.69442171e-01, -7.08023757e-02, -3.55170779e-02,\n",
       "        -1.93730935e-01,  2.04810396e-01, -9.94495153e-02,  1.07296808e-02,\n",
       "        -1.08115502e-01, -7.18033537e-02, -5.75377420e-02, -3.71945262e-01,\n",
       "        -1.46059655e-02, -1.28043860e-01, -2.40071639e-01, -8.83483589e-02,\n",
       "        -6.44060150e-02, -2.64837500e-02, -3.36582623e-02, -5.57616167e-02,\n",
       "        -3.72597992e-01,  4.04476315e-01, -6.05578348e-02,  0.00000000e+00,\n",
       "        -4.03775014e-02, -1.32861614e-01, -5.04497886e-02, -4.37833726e-01,\n",
       "        -8.75015184e-02, -3.03550623e-02, -5.79702199e-01, -4.10032123e-02,\n",
       "        -2.25452520e-02, -2.01855637e-02, -1.31949976e-01,  0.00000000e+00,\n",
       "        -5.04125431e-02, -5.75503819e-02,  1.88489169e-01, -3.66644897e-02,\n",
       "        -2.34254859e-02,  6.23802841e-01, -1.01747906e+00, -5.33991084e-02,\n",
       "        -2.04347238e-01, -1.77858733e-02, -5.93755022e-02, -2.62824655e-01,\n",
       "        -8.34343582e-02, -8.55951160e-02, -4.03440334e-02, -7.77108446e-02,\n",
       "        -7.79360384e-02, -7.32614025e-02, -8.22321698e-02,  1.93940282e-01,\n",
       "        -7.84078389e-02, -8.49651918e-02, -5.22227809e-02, -3.36890936e-01,\n",
       "         4.69419779e-03, -6.55351639e-01,  6.84419751e-01, -2.39652880e-02,\n",
       "        -9.72232372e-02, -7.93786943e-01, -5.74728996e-02, -1.41544268e-02,\n",
       "        -8.04839935e-03, -5.10511547e-02, -5.91009967e-02, -2.57265680e-02,\n",
       "         5.87729752e-01, -4.75529544e-02, -7.79641792e-02, -7.31656924e-02,\n",
       "        -7.75404572e-02, -9.11326334e-02, -7.23493546e-02, -1.28283342e-02,\n",
       "        -1.04427300e-01, -4.43723425e-02, -3.89261469e-02, -6.36364043e-01,\n",
       "        -1.71193648e-02, -1.20211639e-01, -4.68280017e-02, -1.71142239e-02,\n",
       "        -5.77975027e-02, -3.83037478e-02, -5.08522578e-02,  3.11396927e-01,\n",
       "        -1.63660161e-02,  4.76376303e-02, -9.32619572e-01, -1.02719545e-01,\n",
       "         2.71028399e-01, -6.80647045e-02, -9.85433936e-01, -5.54963807e-03,\n",
       "        -6.09225174e-03, -2.52277199e-02, -3.32299322e-02, -8.42391908e-01,\n",
       "         2.13319007e-02, -6.32048994e-02, -9.96438637e-02, -1.65761597e-02,\n",
       "        -8.25781643e-01, -3.40018660e-01, -6.24587424e-02, -2.29750827e-01,\n",
       "         3.50479841e-01,  1.18032187e-01, -7.84992203e-02, -8.72323662e-02,\n",
       "        -1.28537491e-01, -2.27147155e-02, -1.47184720e-02,  3.23249876e-01,\n",
       "        -5.89051880e-02, -6.94792271e-01, -9.14426804e-01, -4.97772396e-02,\n",
       "        -4.72769439e-02, -2.64303237e-02, -5.37839867e-02, -1.77503712e-02,\n",
       "        -6.48781285e-02, -1.89515334e-02, -5.51849365e-01,  7.38053501e-01,\n",
       "        -8.35147351e-02, -1.07035182e-01,  2.56494313e-01, -3.62876728e-02,\n",
       "        -4.36727852e-02, -4.30888720e-02, -6.35575235e-01, -7.17492029e-02,\n",
       "        -1.24668377e-02,  3.66602838e-01, -1.00298367e-01,  8.65129650e-01,\n",
       "        -6.18259013e-01, -7.74070844e-02, -1.12268925e+00, -5.77475876e-02,\n",
       "         7.52874076e-01, -1.93324257e-02, -4.81726527e-02, -1.15168262e-02,\n",
       "         1.77197441e-01, -1.01505136e+00,  4.69839633e-01, -4.62918341e-01,\n",
       "        -3.05264797e-02, -1.16056968e-02, -6.15818128e-02, -4.82762121e-02,\n",
       "        -1.05269738e-01, -1.60856977e-01, -5.47058761e-01, -6.18096367e-02,\n",
       "        -6.38191700e-02,  3.54551613e-01, -5.08062243e-02, -1.40474853e-03,\n",
       "         0.00000000e+00, -1.28599286e-01, -1.25919318e+00, -7.20034093e-02,\n",
       "        -1.02321975e-01, -1.80803835e-01, -9.84558836e-02, -5.98962426e-01,\n",
       "        -6.50889099e-01, -9.08794999e-02, -9.73602593e-01, -4.03337985e-01,\n",
       "        -7.12757483e-02, -2.93371845e-02, -4.25646119e-02, -7.58446395e-01,\n",
       "        -6.81516901e-02,  6.12411574e-02, -4.26957086e-02, -3.01990919e-02,\n",
       "         1.65441170e-01, -9.59218740e-01, -5.73790334e-02, -1.42344264e-02,\n",
       "        -8.65363404e-02, -1.46366134e-02, -3.17257315e-01, -2.84169102e-03,\n",
       "        -2.15228740e-02, -2.10422710e-01, -1.04755771e+00, -5.80586009e-02,\n",
       "        -7.69623518e-01, -2.49656253e-02,  9.01791632e-01, -2.69594230e-02,\n",
       "        -5.65127060e-02, -4.08465974e-02, -8.02377835e-02, -4.27814536e-02,\n",
       "        -4.42784093e-03, -3.28179263e-02, -2.58189261e-01, -3.78699079e-02,\n",
       "        -2.28023622e-02, -8.37559402e-02,  1.08964169e+00, -8.56431946e-02,\n",
       "        -7.05224797e-02,  0.00000000e+00, -3.93192440e-01, -1.44721806e-01,\n",
       "        -4.76173013e-02, -3.50095153e-01, -2.27547716e-02, -7.58033693e-02,\n",
       "        -3.15779485e-02,  4.41280782e-01, -3.00303370e-01, -1.26166670e-02,\n",
       "        -1.11512542e-01,  1.56806201e-01, -4.46629643e-01,  0.00000000e+00],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model.layers[1].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler,'models/scaler3.pkl');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
